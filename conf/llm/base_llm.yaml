llm_model: null                                                 # The name of the LLM model to use
temperature: 0.7                                                # The temperature to use for sampling
max_attempts: 10                                                # The maximum number of attempts to query the LLM for a valid response
debug: False                                                    # Whether to enable debug mode for the LLM (see LLM agent for specific behavior)
sleep_time: 5                                                   # The time to sleep between LLM queries
max_feedback_steps: 5                                           # The maximum number of times to provide feedback for the LLM agent (before retrying)
feedback_attempts: 3                                            # The number of attempts to exceed feedback for the LLM agent (terminates when met)

# [Reflexion only]
retries: 0                                                      # The number of times to reset the environment and try again if the LLM agent fails

# In-context examples
num_examples: 0                                                 # The number of in-context examples to use for the LLM agent
example_dir_path: null                                          # The directory containing the in-context examples

# GPT Cost Estimator
mock: False                                                     # [gpt-cost-estimator] Whether to mock API calls
completion_tokens: 1                                            # [gpt-cost-estimator] The number of completion tokens to output for a mocked call

# Huggingface Pipeline
max_length: 8192        # The maximum number of tokens for the input to the LLM model

# Logging
log_path: ${hydra:runtime.output_dir}/in-context_example.txt    # The name of the file to save the log to

# Stochasticity
failure_probability: 0.0                                        # The probability of a failure occurring when querying the LLM agent

# Prompts
prompts:
  action_proposal_prompt:
    experiment_name: null
    prompt_description: null
    prompt_version: null
    model: ${llm.llm_model}
    temperature: ${llm.temperature}
    max_attempts: ${llm.max_attempts}
    debug: ${llm.debug}
    sleep_time: ${llm.sleep_time}
    mock: ${llm.mock}
    completion_tokens: ${llm.completion_tokens}
llm_model: null         # The name of the LLM model to use
temperature: 0.7        # The temperature to use for sampling
max_attempts: 10        # The maximum number of attempts to query the LLM for a valid response
debug: False            # Whether to enable debug mode for the LLM (see LLM agent for specific behavior)
sleep_time: 5           # The time to sleep between LLM queries
max_feedback_steps: 5   # The maximum number of times to provide feedback for the LLM agent (before terminating)

# In-context examples
num_examples: 0         # The number of in-context examples to use for the LLM agent
example_dir_path: null  # The directory containing the in-context examples

# GPT Cost Estimator
mock: False             # [gpt-cost-estimator] Whether to mock API calls
completion_tokens: 1    # [gpt-cost-estimator] The number of completion tokens to output for a mocked call

# Logging
log_path: null          # The name of the file to save the log to

# Prompts
prompts:
  action_proposal_prompt:
    experiment_name: null
    prompt_description: null
    prompt_version: null
    model: ${llm.llm_model}
    temperature: ${llm.temperature}
    max_attempts: ${llm.max_attempts}
    debug: ${llm.debug}
    sleep_time: ${llm.sleep_time}
    mock: ${llm.mock}
    completion_tokens: ${llm.completion_tokens}